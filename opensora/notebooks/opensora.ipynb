{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b916cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"NVIDIA_VISIBLE_DEVICES\"]=\"\"\n",
    "from opensora.acceleration.parallel_states import get_data_parallel_group\n",
    "from opensora.datasets.dataloader import prepare_dataloader\n",
    "from opensora.registry import DATASETS, build_module\n",
    "from opensora.utils.cai import (\n",
    "    get_booster,\n",
    "    get_is_saving_process,\n",
    "    init_inference_environment,\n",
    ")\n",
    "from opensora.utils.config import parse_alias, parse_configs\n",
    "from opensora.utils.inference import (\n",
    "    add_fps_info_to_text,\n",
    "    add_motion_score_to_text,\n",
    "    create_tmp_csv,\n",
    "    modify_option_to_t2i,\n",
    "    process_and_save,\n",
    ")\n",
    "from opensora.utils.logger import create_logger, is_main_process\n",
    "from opensora.utils.misc import log_cuda_max_memory, to_torch_dtype\n",
    "from opensora.utils.prompt_refine import refine_prompts\n",
    "from opensora.utils.sampling import (\n",
    "    SamplingOption,\n",
    "    prepare_api,\n",
    "    prepare_models,\n",
    "    sanitize_sampling_option,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import snapshot_download\n",
    "from colossalai.utils import set_seed\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "from pprint import pformat\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe66b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148b99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1. configs & runtime variables\n",
    "# ======================================================\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# == parse configs ==\n",
    "cfg = parse_configs(\"../configs/diffusion/inference/t2i2v_256px.py\")\n",
    "cfg = parse_alias(cfg)\n",
    "cfg[\"prompt\"] = \"raining, sea\"\n",
    "cfg[\"ref\"] = \"../assets/texts/i2v.png\"\n",
    "\n",
    "model_path = Path(\"models\")\n",
    "if not model_path.exists():\n",
    "    model_path.mkdir()\n",
    "    REPO_ID = \"hpcai-tech/Open-Sora-v2\"\n",
    "    snapshot_download(repo_id=REPO_ID, local_dir=model_path)\n",
    "\n",
    "\n",
    "# == device and dtype ==\n",
    "\n",
    "device = \"cpu\"  # âˆ‚ if not torch.cuda.is_available() else \"cuda\"\n",
    "dtype = to_torch_dtype(cfg.get(\"dtype\", \"bf16\"))\n",
    "seed = cfg.get(\"seed\", 1024)\n",
    "if seed is not None:\n",
    "    set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c2f79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/30/25 20:08:32] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> colossalai - colossalai - INFO:                                                       \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/opt/conda/envs/opensora/lib/python3.10/site-packages/colossalai/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">initialize.py</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75</span>     \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         launch                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/30/25 20:08:32]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO:                                                       \n",
       "\u001b[2;36m                    \u001b[0m         \u001b[35m/opt/conda/envs/opensora/lib/python3.10/site-packages/colossalai/\u001b[0m\u001b[95minitialize.py\u001b[0m:\u001b[1;36m75\u001b[0m     \n",
       "\u001b[2;36m                    \u001b[0m         launch                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> colossalai - colossalai - INFO: Distributed environment is initialized, world size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m colossalai - colossalai - INFO: Distributed environment is initialized, world size: \u001b[1;36m1\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# == init distributed env ==\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "init_inference_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32de4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-07-30 20:08:34\u001b[0m] Inference configuration:\n",
      " {'ae': {'from_pretrained': 'models/hunyuan_vae.safetensors',\n",
      "        'in_channels': 3,\n",
      "        'latent_channels': 16,\n",
      "        'layers_per_block': 2,\n",
      "        'out_channels': 3,\n",
      "        'type': 'hunyuan_vae',\n",
      "        'use_spatial_tiling': True,\n",
      "        'use_temporal_tiling': False},\n",
      " 'batch_size': 1,\n",
      " 'clip': {'from_pretrained': 'models/openai/clip-vit-large-patch14',\n",
      "          'max_length': 77,\n",
      "          'type': 'text_embedder'},\n",
      " 'cond_type': 't2v',\n",
      " 'config_path': '../configs/diffusion/inference/t2i2v_256px.py',\n",
      " 'dataset': {'type': 'text'},\n",
      " 'dtype': 'bf16',\n",
      " 'fps_save': 24,\n",
      " 'img_flux': {'axes_dim': [16, 56, 56],\n",
      "              'cond_embed': False,\n",
      "              'context_in_dim': 4096,\n",
      "              'depth': 19,\n",
      "              'depth_single_blocks': 38,\n",
      "              'from_pretrained': './models/flux1-dev.safetensors',\n",
      "              'guidance_embed': True,\n",
      "              'hidden_size': 3072,\n",
      "              'in_channels': 64,\n",
      "              'mlp_ratio': 4.0,\n",
      "              'num_heads': 24,\n",
      "              'qkv_bias': True,\n",
      "              'theta': 10000,\n",
      "              'type': 'flux',\n",
      "              'vec_in_dim': 768},\n",
      " 'img_flux_ae': {'ch': 128,\n",
      "                 'ch_mult': [1, 2, 4, 4],\n",
      "                 'from_pretrained': 'models/flux1-dev-ae.safetensors',\n",
      "                 'in_channels': 3,\n",
      "                 'num_res_blocks': 2,\n",
      "                 'out_ch': 3,\n",
      "                 'resolution': 256,\n",
      "                 'scale_factor': 0.3611,\n",
      "                 'shift_factor': 0.1159,\n",
      "                 'type': 'autoencoder_2d',\n",
      "                 'z_channels': 16},\n",
      " 'img_resolution': '768px',\n",
      " 'model': {'axes_dim': [16, 56, 56],\n",
      "           'cond_embed': True,\n",
      "           'context_in_dim': 4096,\n",
      "           'depth': 19,\n",
      "           'depth_single_blocks': 38,\n",
      "           'from_pretrained': 'models/Open_Sora_v2.safetensors',\n",
      "           'fused_qkv': False,\n",
      "           'guidance_embed': False,\n",
      "           'hidden_size': 3072,\n",
      "           'in_channels': 64,\n",
      "           'mlp_ratio': 4.0,\n",
      "           'num_heads': 24,\n",
      "           'qkv_bias': True,\n",
      "           'theta': 10000,\n",
      "           'type': 'flux',\n",
      "           'use_liger_rope': True,\n",
      "           'vec_in_dim': 768},\n",
      " 'motion_score': '4',\n",
      " 'prompt': 'raining, sea',\n",
      " 'ref': '../assets/texts/i2v.png',\n",
      " 'sampling_option': {'aspect_ratio': '16:9',\n",
      "                     'guidance': 7.5,\n",
      "                     'guidance_img': 3.0,\n",
      "                     'image_osci': True,\n",
      "                     'is_causal_vae': True,\n",
      "                     'method': 'i2v',\n",
      "                     'num_frames': 129,\n",
      "                     'num_steps': 50,\n",
      "                     'resolution': '256px',\n",
      "                     'scale_temporal_osci': True,\n",
      "                     'seed': None,\n",
      "                     'shift': True,\n",
      "                     'temporal_reduction': 4,\n",
      "                     'text_osci': True},\n",
      " 'save_dir': 'samples',\n",
      " 'seed': 42,\n",
      " 't5': {'from_pretrained': 'models/google/t5-v1_1-xxl',\n",
      "        'max_length': 512,\n",
      "        'shardformer': True,\n",
      "        'type': 'text_embedder'},\n",
      " 'use_t2i2v': True}\n"
     ]
    }
   ],
   "source": [
    "logger = create_logger()\n",
    "logger.info(\"Inference configuration:\\n %s\", pformat(cfg.to_dict()))\n",
    "is_saving_process = get_is_saving_process(cfg)\n",
    "booster = get_booster(cfg)\n",
    "booster_ae = get_booster(cfg, ae=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d4ce984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-07-30 20:08:39\u001b[0m] Building dataset...\n",
      "[\u001b[34m2025-07-30 20:08:39\u001b[0m] Dataset contains 1 samples.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# 2. build dataset and dataloader\n",
    "# ======================================================\n",
    "logger.info(\"Building dataset...\")\n",
    "\n",
    "# save directory\n",
    "save_dir = cfg.save_dir\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# == build dataset ==\n",
    "if cfg.get(\"prompt\"):\n",
    "    cfg.dataset.data_path = create_tmp_csv(\n",
    "        save_dir, cfg.prompt, cfg.get(\"ref\", None), create=is_main_process()\n",
    "    )\n",
    "dist.barrier()\n",
    "dataset = build_module(cfg.dataset, DATASETS)\n",
    "\n",
    "# range selection\n",
    "start_index = cfg.get(\"start_index\", 0)\n",
    "end_index = cfg.get(\"end_index\", None)\n",
    "if end_index is None:\n",
    "    end_index = start_index + cfg.get(\"num_samples\", len(dataset.data) + 1)\n",
    "dataset.data = dataset.data[start_index:end_index]\n",
    "logger.info(\"Dataset contains %s samples.\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37afa4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sampling_interval': 1,\n",
       " 'text': 'raining, sea',\n",
       " 'ref': '../assets/texts/i2v.png',\n",
       " 'index': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6936ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == build dataloader ==\n",
    "dataloader_args = dict(\n",
    "    dataset=dataset,\n",
    "    batch_size=cfg.get(\"batch_size\", 1),\n",
    "    num_workers=cfg.get(\"num_workers\", 4),\n",
    "    seed=cfg.get(\"seed\", 1024),\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    pin_memory=True,\n",
    "    process_group=get_data_parallel_group(),\n",
    "    prefetch_factor=cfg.get(\"prefetch_factor\", None),\n",
    ")\n",
    "dataloader, _ = prepare_dataloader(**dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab76543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == prepare default params ==\n",
    "sampling_option = SamplingOption(**cfg.sampling_option)\n",
    "sampling_option = sanitize_sampling_option(sampling_option)\n",
    "\n",
    "cond_type = cfg.get(\"cond_type\", \"i2v_head\")\n",
    "prompt_refine = cfg.get(\"prompt_refine\", False)\n",
    "fps_save = cfg.get(\"fps_save\", 16)\n",
    "num_sample = cfg.get(\"num_sample\", 1)\n",
    "\n",
    "type_name = \"image\" if cfg.sampling_option.num_frames == 1 else \"video\"\n",
    "sub_dir = f\"{type_name}_{cfg.sampling_option.resolution}\"\n",
    "os.makedirs(os.path.join(save_dir, sub_dir), exist_ok=True)\n",
    "use_t2i2v = cfg.get(\"use_t2i2v\", False)\n",
    "img_sub_dir = os.path.join(sub_dir, \"generated_condition\")\n",
    "if use_t2i2v:\n",
    "    os.makedirs(os.path.join(save_dir, sub_dir, \"generated_condition\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "410995ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-07-30 20:15:17\u001b[0m] Building models...\n",
      "[\u001b[34m2025-07-30 20:16:17\u001b[0m] Loading checkpoint from models/Open_Sora_v2.safetensors\n",
      "[\u001b[34m2025-07-30 20:17:12\u001b[0m] Model loaded successfully\n",
      "[\u001b[34m2025-07-30 20:17:15\u001b[0m] Loading checkpoint from models/hunyuan_vae.safetensors\n",
      "[\u001b[34m2025-07-30 20:17:17\u001b[0m] Model loaded successfully\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7528a152d9b7433a9adf050260ec69c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-07-30 20:20:24\u001b[0m] Loading checkpoint from ./models/flux1-dev.safetensors\n",
      "[\u001b[34m2025-07-30 20:20:45\u001b[0m] Model loaded successfully\n",
      "[\u001b[34m2025-07-30 20:20:46\u001b[0m] Loading checkpoint from models/flux1-dev-ae.safetensors\n",
      "[\u001b[34m2025-07-30 20:20:46\u001b[0m] Model loaded successfully\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# == build flux model ==\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model, model_ae, model_t5, model_clip, optional_models \u001b[38;5;241m=\u001b[39m prepare_models(\n\u001b[1;32m      8\u001b[0m     cfg, device, dtype, offload_model\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffload_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mlog_cuda_max_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbuild model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/config/workspace/opensora/opensora/utils/misc.py:63\u001b[0m, in \u001b[0;36mlog_cuda_max_memory\u001b[0;34m(stage)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlog_cuda_max_memory\u001b[39m(stage: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    Log the max CUDA memory usage.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m        stage (str): The stage of the training process.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     max_memory_allocated \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_allocated()\n\u001b[1;32m     65\u001b[0m     max_memory_reserved \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_reserved()\n",
      "File \u001b[0;32m/opt/conda/envs/opensora/lib/python3.10/site-packages/torch/cuda/__init__.py:890\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msynchronize\u001b[39m(device: _device_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    883\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for all kernels in all streams on a CUDA device to complete.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m            if :attr:`device` is ``None`` (default).\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 890\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_synchronize()\n",
      "File \u001b[0;32m/opt/conda/envs/opensora/lib/python3.10/site-packages/torch/cuda/__init__.py:314\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    313\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 314\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    318\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# 3. build model\n",
    "# ======================================================\n",
    "logger.info(\"Building models...\")\n",
    "\n",
    "# == build flux model ==\n",
    "model, model_ae, model_t5, model_clip, optional_models = prepare_models(\n",
    "    cfg, device, dtype, offload_model=cfg.get(\"offload_model\", True)\n",
    ")\n",
    "log_cuda_max_memory(\"build model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4defe52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_fn = prepare_api(model, model_ae, model_t5, model_clip, optional_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16a8c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image flux model if t2i2v\n",
    "if use_t2i2v:\n",
    "    api_fn_img = prepare_api(\n",
    "        optional_models[\"img_flux\"],\n",
    "        optional_models[\"img_flux_ae\"],\n",
    "        model_t5,\n",
    "        model_clip,\n",
    "        optional_models,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 4. inference\n",
    "# ======================================================\n",
    "for epoch in range(num_sample):  # generate multiple samples with different seeds\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    with tqdm(\n",
    "        enumerate(dataloader_iter, start=0),\n",
    "        desc=\"Inference progress\",\n",
    "        disable=not is_main_process(),\n",
    "        initial=0,\n",
    "        total=len(dataloader),\n",
    "    ) as pbar:\n",
    "        for _, batch in pbar:\n",
    "            original_text = batch.pop(\"text\")\n",
    "            if use_t2i2v:\n",
    "                batch[\"text\"] = (\n",
    "                    original_text\n",
    "                    if not prompt_refine\n",
    "                    else refine_prompts(original_text, type=\"t2i\")\n",
    "                )\n",
    "                sampling_option_t2i = modify_option_to_t2i(\n",
    "                    sampling_option,\n",
    "                    distilled=True,\n",
    "                    img_resolution=cfg.get(\"img_resolution\", \"768px\"),\n",
    "                )\n",
    "                if cfg.get(\"offload_model\", False):\n",
    "                    model_move_start = time.time()\n",
    "                    model = model.to(\"cpu\", dtype)\n",
    "                    model_ae = model_ae.to(\"cpu\", dtype)\n",
    "                    optional_models[\"img_flux\"].to(device, dtype)\n",
    "                    optional_models[\"img_flux_ae\"].to(device, dtype)\n",
    "                    logger.info(\n",
    "                        \"offload video diffusion model to cpu, load image flux model to gpu: %s s\",\n",
    "                        time.time() - model_move_start,\n",
    "                    )\n",
    "\n",
    "                logger.info(\"Generating image condition by flux...\")\n",
    "                x_cond = api_fn_img(\n",
    "                    sampling_option_t2i,\n",
    "                    \"t2v\",\n",
    "                    seed=sampling_option.seed + epoch if sampling_option.seed else None,\n",
    "                    channel=cfg[\"img_flux\"][\"in_channels\"],\n",
    "                    **batch,\n",
    "                ).cpu()\n",
    "\n",
    "                # save image to disk\n",
    "                batch[\"name\"] = process_and_save(\n",
    "                    x_cond,\n",
    "                    batch,\n",
    "                    cfg,\n",
    "                    img_sub_dir,\n",
    "                    sampling_option_t2i,\n",
    "                    epoch,\n",
    "                    start_index,\n",
    "                    saving=is_saving_process,\n",
    "                )\n",
    "                dist.barrier()\n",
    "\n",
    "                if cfg.get(\"offload_model\", True):\n",
    "                    model_move_start = time.time()\n",
    "                    model = model.to(device, dtype)\n",
    "                    model_ae = model_ae.to(device, dtype)\n",
    "                    optional_models[\"img_flux\"].to(\"cpu\", dtype)\n",
    "                    optional_models[\"img_flux_ae\"].to(\"cpu\", dtype)\n",
    "                    logger.info(\n",
    "                        \"load video diffusion model to gpu, offload image flux model to cpu: %s s\",\n",
    "                        time.time() - model_move_start,\n",
    "                    )\n",
    "\n",
    "                ref_dir = os.path.join(\n",
    "                    save_dir, os.path.join(sub_dir, \"generated_condition\")\n",
    "                )\n",
    "                batch[\"ref\"] = [\n",
    "                    os.path.join(ref_dir, f\"{x}.png\") for x in batch[\"name\"]\n",
    "                ]\n",
    "                cond_type = \"i2v_head\"\n",
    "\n",
    "            batch[\"text\"] = original_text\n",
    "            if prompt_refine:\n",
    "                batch[\"text\"] = refine_prompts(\n",
    "                    original_text,\n",
    "                    type=\"t2v\" if cond_type == \"t2v\" else \"t2i\",\n",
    "                    image_paths=batch.get(\"ref\", None),\n",
    "                )\n",
    "            batch[\"text\"] = add_fps_info_to_text(batch.pop(\"text\"), fps=fps_save)\n",
    "            if \"motion_score\" in cfg:\n",
    "                batch[\"text\"] = add_motion_score_to_text(\n",
    "                    batch.pop(\"text\"), cfg.get(\"motion_score\", 5)\n",
    "                )\n",
    "\n",
    "            logger.info(\"Generating video...\")\n",
    "            x = api_fn(\n",
    "                sampling_option,\n",
    "                cond_type,\n",
    "                seed=sampling_option.seed + epoch if sampling_option.seed else None,\n",
    "                patch_size=cfg.get(\"patch_size\", 2),\n",
    "                save_prefix=cfg.get(\"save_prefix\", \"\"),\n",
    "                channel=cfg[\"model\"][\"in_channels\"],\n",
    "                **batch,\n",
    "            ).cpu()\n",
    "\n",
    "            if is_saving_process:\n",
    "                process_and_save(\n",
    "                    x, batch, cfg, sub_dir, sampling_option, epoch, start_index\n",
    "                )\n",
    "            dist.barrier()\n",
    "\n",
    "logger.info(\"Inference finished.\")\n",
    "log_cuda_max_memory(\"inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baeda0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sampling_interval': tensor([1]),\n",
       " 'ref': ['../assets/texts/i2v.png'],\n",
       " 'index': tensor([0]),\n",
       " 'text': ['raining, sea']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f72f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
